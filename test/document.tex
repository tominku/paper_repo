\documentclass[12pt]{article}

\usepackage{answers}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{multicol}
\usepackage{mathrsfs}
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
 
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}

\newcommand{\R}{\mathbb{R}}


\begin{document}
 
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------
  
\title{Some Derivations... test}

\author{MinKu Kang}
 
\maketitle

\begin{align}
\max \; &\left[ - H(x_{t+1}) + H(x_{t}) \right] \approx 
\max \; | \dot{x}_t  | \Delta t = v_{max} \Delta t \\
%\max \Delta H
\end{align}

\begin{align*}
R^{max} = &r_0^{max} &+ &r_1^{max} &+& ...  + r_{T^*}^{max} &+ &c_{T^{*}+1} + ... + c_{\infty} \\
R' = &r_0 &+ &r_1 &+& ... + r_{T^*} &+ &r_{T^{*}+1} + r_{T^{*}+2} + ... + r_{T^{*}+k} + c_{T^{*}+k+1} + ... + c_{\infty}  
\end{align*}

\begin{align*}
R^{max} - R' = \underbrace{(r_0^{max} - r_0) + (r_1^{max} - r_1) + ... + (r_{T^*}^{max} - r_{T^*})}_{\geq \; 0} + \\
\underbrace{(c_{T^{*}+1} - r_{T^{*}+1}) + (c_{T^{*}+2} - r_{T^{*}+2}) + ... + (c_{T^{*}+k} - r_{T^{*}+k})}_{\geq \; 0 ? }
\end{align*}

One simple solution to make the second part always greater than zero is:
\begin{align*}
c_t > r_t \;\; \forall t. \\
c_t := r^{max}
\end{align*}
Rather, one would be tempted to set $c$ arbitrary large, but this can make the return arbitrary large, adversely affecting learning stability. Thus, we want to find an accurate estimate of the max reward, $\hat{r}^{max}$ to set $c$ appropriately.

\begin{align*}
R_0^{max} = r_0^* + r_1^* + ... + r_{T^*}^* \\
R_0 = r_0 + r_1 + ... + r_{T^*}
\end{align*}

\begin{align*}
R_0^{max} - R_0 = r_0^* - r_0 + R_1^{max} - R_1 > 0 \\
\text{by the Priciple of Optimality,} \\
R_1^{max} - R_1 > 0. \\
\text{And note that,} \\
r_0^* - r_0 > - (R_1^{max} - R_1) < 0 
\end{align*}
This implies that $r_0^* - r_0$ can be negative, meaning that $r_0$ can be larger than $r_0^*$ 

\begin{align*}
R_0^{max} - R_0 = (r_0^{*} - r_0) + (r_1^{*} - r_1) + ... + (r_{T^*}^{*} - r_{T^*}) + \\
\underbrace{(c_{T^{*}+1} - r_{T^{*}+1}) + (c_{T^{*}+2} - r_{T^{*}+2}) 
	+ ... + (c_{T^{*}+k} - r_{T^{*}+k})}_{ D_{1:k} }
\end{align*}

By the Principle of Optimality, the following conditions should be satisfied:
\begin{align*}
D_{k:k} > 0 \\
D_{k-1:k} > 0 \\
\ldots \\
D_{1:k} > 0 \\
\end{align*}
A simple setting that can satisfy the above conditions is to set $c_t$ as:

\begin{align*}
c_t := r^{max} \;\; \forall t.
\end{align*}

\noindent However, it is sometimes not trivial to find $r^{max}$, which might require the knowledge of dynamics model of the environment. For now, let us assume that we know $r^{max}$. 
\\

\noindent Now, let us consider the Reward Shaping term:
\begin{align*}
F(s_{t+1}, s_t) = \gamma \phi(s_{t+1}) - \phi(s_{t}). 
\end{align*}
\noindent This can be further expanded such that:
\begin{align*}
F(s_{t+1}, s_t) &= \phi(s_{t+1}) - \phi(s_{t}) + \gamma \phi(s_{t+1}) - \phi(s_{t+1}) \\
&= \phi(s_{t+1}) - \phi(s_{t}) + (\gamma - 1) \phi(s_{t+1})
\end{align*}
Let us design a reward function such that:
\begin{align*}
r(s_t, a_t, s_{t+1}) &= r_{original}(s_t, a_t, s_{t+1}) + F_t \\
r_{original}(s_t, a_t, s_{t+1}) &:= -(\gamma - 1) \phi(s_{t+1}) = \alpha \phi(s_{t+1}), \; \alpha > 0 \\
\implies r(s_t, a_t, s_{t+1}) &= \phi(s_{t+1}) - \phi(s_{t})
\end{align*}
Since the shaping term $F_t$ does not affect the optimal policy, the optimal policy will maximize the discounted sum of $r_{original}$:
\begin{align*}
\sum_{i=0}^{\infty} \gamma^i \: r_{original}(s_i, a_i, s_{i+1}) = \sum_{i=0}^{\infty} \gamma^i \: \alpha \phi(s_{t+1}) = \alpha \sum_{i=0}^{\infty} \gamma^i \: \phi(s_{t+1})
\end{align*}

\end{document}
\grid
